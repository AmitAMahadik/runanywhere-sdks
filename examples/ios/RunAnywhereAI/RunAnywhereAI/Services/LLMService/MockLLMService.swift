//
//  MockLLMService.swift
//  RunAnywhereAI
//
//  Created on 7/26/25.
//

import Foundation

class MockLLMService: LLMService {
    var name: String = "Mock LLM"
    var isInitialized: Bool = true
    
    var supportedModels: [ModelInfo] = [
        ModelInfo(
            name: "Mock Model",
            size: "10MB",
            framework: "Mock",
            quantization: nil,
            description: "A mock model for testing",
            minimumMemory: 100_000_000,
            recommendedMemory: 200_000_000
        )
    ]
    
    private var currentModel: ModelInfo?
    
    func initialize(modelPath: String) async throws {
        // Simulate initialization delay
        try await Task.sleep(nanoseconds: 500_000_000) // 0.5 seconds
        currentModel = supportedModels.first
        isInitialized = true
    }
    
    func generate(prompt: String, options: GenerationOptions) async throws -> String {
        guard isInitialized else {
            throw LLMError.notInitialized
        }
        
        // Simulate generation delay
        try await Task.sleep(nanoseconds: 1_000_000_000) // 1 second
        
        // Return a mock response
        return "This is a mock response to your prompt: '\(prompt)'. " +
               "In a real implementation, this would be generated by an actual LLM."
    }
    
    func streamGenerate(
        prompt: String,
        options: GenerationOptions,
        onToken: @escaping (String) -> Void
    ) async throws {
        guard isInitialized else {
            throw LLMError.notInitialized
        }
        
        let response = "This is a streaming mock response to: '\(prompt)'."
        let words = response.split(separator: " ")
        
        for word in words {
            try await Task.sleep(nanoseconds: 100_000_000) // 0.1 seconds per word
            onToken(String(word) + " ")
        }
    }
    
    func getModelInfo() -> ModelInfo? {
        return currentModel
    }
    
    func cleanup() {
        isInitialized = false
        currentModel = nil
    }
}
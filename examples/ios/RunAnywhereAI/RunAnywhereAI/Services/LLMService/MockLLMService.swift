//
//  MockLLMService.swift
//  RunAnywhereAI
//
//  Created by Sanchit Monga on 7/26/25.
//

import Foundation

class MockLLMService: LLMService {
    
    // MARK: - Properties
    
    private var currentModel: ModelInfo?
    var modelState: ModelState = .unloaded
    var generationState: GenerationState = .idle
    private var isStreaming = false
    
    // MARK: - LLMService Protocol
    
    var frameworkInfo: FrameworkInfo {
        FrameworkInfo(
            name: "Mock LLM",
            version: "1.0",
            developer: "RunAnywhere",
            description: "A mock LLM service for testing and development",
            website: nil,
            documentation: nil,
            minimumOSVersion: "13.0",
            requiredCapabilities: [],
            optimizedFor: [.lowLatency, .memoryEfficient],
            features: [.onDeviceInference, .offlineCapable]
        )
    }
    
    var name: String { "Mock LLM" }
    
    var isInitialized: Bool {
        if case .loaded = modelState {
            return true
        }
        return false
    }
    
    var supportedModels: [ModelInfo] = [
        ModelInfo(
            name: "Mock Model",
            format: .other,
            size: "10MB",
            framework: .mock,
            quantization: nil,
            description: "A mock model for testing",
            minimumMemory: 100_000_000,
            recommendedMemory: 200_000_000
        )
    ]
    
    // MARK: - LLMCapabilities
    
    var supportsStreaming: Bool { true }
    var supportsQuantization: Bool { false }
    var supportsBatching: Bool { false }
    var supportsMultiModal: Bool { false }
    var quantizationFormats: [QuantizationFormat] { [] }
    var maxContextLength: Int { 2048 }
    var supportsCustomOperators: Bool { false }
    var hardwareAcceleration: [HardwareAcceleration] { [.cpu] }
    
    // MARK: - LLMModelLoader
    
    var supportedFormats: [ModelFormat] { [.other] }
    
    func isFormatSupported(_ format: ModelFormat) -> Bool {
        format == .other
    }
    
    func loadModel(_ path: String) async throws {
        modelState = .loading(progress: 0.0)
        
        // Simulate loading progress
        for progress in stride(from: 0.0, through: 1.0, by: 0.2) {
            modelState = .loading(progress: progress)
            try await Task.sleep(nanoseconds: 100_000_000) // 0.1s
        }
        
        currentModel = supportedModels.first
        modelState = .loaded(modelInfo: currentModel!)
    }
    
    func unloadModel() async throws {
        currentModel = nil
        modelState = .unloaded
    }
    
    func preloadModel(_ config: ModelConfiguration) async throws {
        try await loadModel(config.modelPath)
    }
    
    func validateModel(at path: String) async throws -> ModelValidation {
        ModelValidation(
            isValid: true,
            format: .other,
            fileSize: 10_000_000,
            estimatedMemory: 100_000_000,
            warnings: [],
            metadata: ["type": "mock"]
        )
    }
    
    // MARK: - LLMInference
    
    var isReadyForInference: Bool { isInitialized }
    
    func generate(_ request: GenerationRequest) async throws -> GenerationResponse {
        guard isInitialized else {
            throw LLMError.notInitialized(service: name)
        }
        
        generationState = .generating(progress: GenerationProgress(
            tokensGenerated: 0,
            estimatedTotal: 50,
            currentSpeed: 0,
            elapsedTime: 0
        ))
        
        // Simulate generation delay
        try await Task.sleep(nanoseconds: 1_000_000_000) // 1 second
        
        let response = "This is a mock response to your prompt: '\(request.prompt)'. " +
                      "In a real implementation, this would be generated by an actual LLM."
        
        let result = GenerationResponse(
            text: response,
            tokensGenerated: response.split(separator: " ").count,
            timeToFirstToken: 0.1,
            totalTime: 1.0,
            tokensPerSecond: Double(response.split(separator: " ").count),
            promptTokens: request.prompt.split(separator: " ").count,
            completionTokens: response.split(separator: " ").count,
            totalTokens: request.prompt.split(separator: " ").count + response.split(separator: " ").count,
            finishReason: .completed,
            metadata: ["mock": true]
        )
        
        generationState = .completed(response: result)
        return result
    }
    
    func streamGenerate(_ request: GenerationRequest) -> AsyncThrowingStream<String, Error> {
        AsyncThrowingStream { continuation in
            Task {
                guard isInitialized else {
                    continuation.finish(throwing: LLMError.notInitialized(service: name))
                    return
                }
                
                isStreaming = true
                let response = "This is a streaming mock response to: '\(request.prompt)'."
                let words = response.split(separator: " ")
                
                for (index, word) in words.enumerated() {
                    if !isStreaming {
                        continuation.finish()
                        return
                    }
                    
                    generationState = .generating(progress: GenerationProgress(
                        tokensGenerated: index + 1,
                        estimatedTotal: words.count,
                        currentSpeed: Double(index + 1) / (Double(index + 1) * 0.1),
                        elapsedTime: Double(index + 1) * 0.1
                    ))
                    
                    continuation.yield(String(word) + " ")
                    try? await Task.sleep(nanoseconds: 100_000_000) // 0.1 seconds per word
                }
                
                isStreaming = false
                continuation.finish()
            }
        }
    }
    
    func cancelGeneration() async {
        isStreaming = false
        generationState = .cancelled
    }
    
    // MARK: - LLMMetrics
    
    func getPerformanceMetrics() -> LLMPerformanceMetrics {
        LLMPerformanceMetrics(
            averageTokensPerSecond: 10.0,
            peakTokensPerSecond: 15.0,
            averageLatency: 0.1,
            p95Latency: 0.15,
            p99Latency: 0.2,
            totalTokensGenerated: 1000,
            totalGenerations: 50,
            failureRate: 0.0,
            averageContextLength: 100,
            hardwareUtilization: HardwareUtilization(
                cpuUsage: 10.0,
                gpuUsage: nil,
                neuralEngineUsage: nil,
                powerUsage: 1.0,
                thermalState: .nominal
            )
        )
    }
    
    func getMemoryUsage() -> LLMMemoryStats {
        LLMMemoryStats(
            modelMemory: 10_000_000,
            contextMemory: 1_000_000,
            peakMemory: 11_000_000,
            availableMemory: Int64(ProcessInfo.processInfo.physicalMemory),
            memoryPressure: .normal,
            cacheSize: 0
        )
    }
    
    func getBenchmarkResults() -> BenchmarkResults {
        BenchmarkResults(
            framework: name,
            model: currentModel?.name ?? "Unknown",
            device: "Simulator",
            timestamp: Date(),
            promptProcessingSpeed: 100.0,
            generationSpeed: 10.0,
            firstTokenLatency: 0.1,
            memoryFootprint: 10_000_000,
            energyEfficiency: 0.99,
            qualityScore: 0.5,
            configurations: [:]
        )
    }
    
    func resetMetrics() {
        // No-op for mock
    }
    
    func exportMetrics() -> Data? {
        nil
    }
    
    func subscribeToMetrics(_ handler: @escaping (MetricsUpdate) -> Void) -> UUID {
        UUID()
    }
    
    func unsubscribeFromMetrics(_ id: UUID) {
        // No-op for mock
    }
    
    // MARK: - Additional Protocol Requirements
    
    func getModelInfo() -> ModelInfo? {
        currentModel
    }
    
    func cleanup() {
        isStreaming = false
        Task {
            try? await unloadModel()
        }
    }
    
    func configure(_ options: [String: Any]) throws {
        // No configuration needed for mock
    }
    
    func healthCheck() async -> HealthCheckResult {
        HealthCheckResult(
            isHealthy: true,
            frameworkVersion: frameworkInfo.version,
            availableMemory: Int64(ProcessInfo.processInfo.physicalMemory),
            modelLoaded: isInitialized,
            lastError: nil,
            diagnostics: ["mock": true]
        )
    }
    
    // MARK: - Legacy Support (kept for compatibility)
    
    func initialize(modelPath: String) async throws {
        try await loadModel(modelPath)
    }
    
    func generate(prompt: String, options: GenerationOptions) async throws -> String {
        let request = GenerationRequest(prompt: prompt, options: options)
        let response = try await generate(request)
        return response.text
    }
    
    func streamGenerate(
        prompt: String,
        options: GenerationOptions,
        onToken: @escaping (String) -> Void
    ) async throws {
        let request = GenerationRequest(prompt: prompt, options: options)
        for try await token in streamGenerate(request) {
            onToken(token)
        }
    }
}